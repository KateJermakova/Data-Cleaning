# Data Cleaning

Data cleaning involves identifying and eliminating inaccuracies and inconsistencies from data, which leads to an enhanced data quality. In the dataset there are numerous missing values and categorical features that are in different formats, rendering the data unusable for further machine learning model prediction. Therefore, it is imperative to perform data cleaning on the dataset.

The initial step of the data cleaning process involved addressing the missing values. Non-numeric values in numerical columns, as well as instances in the birth date and bedtime features that were not convertible to one format, were replaced with missing (NaN) values. The next step was to check whether the data was missing at random (MAR), which was indeed the case. Two imputation techniques were used for this purpose. The first one employed k-nearest neighbor (kNN = 5) for numerical values
imputations and mode imputations for categorical ones, while the second one used median imputation for numeric values and mode imputations for categorical ones. The performance of two machine learning models used for classification tasks was then evaluated to compare the outcomes of these two imputation techniques. The Decision Tree classifier that used the option of median imputations for numerical values slightly outperformed the same model using kNN imputations. Modelsâ€™ evaluation metrics were according to the first option: accuracy of 0.6, precision of 0.6, recall of 0.59, and f1- score of 0.59, while the second imputation option had accuracy of 0.58, precision 0.58, recall 0.57 and f1-score 0.57. It can be seen that all evaluation metrics are higher on 0.02 for the model using median imputation. Thus, this approach was used in cleaning the data for further use.

The birth date column contained responses from users in different formats. Therefore, it was necessary to format the date uniformly, adopting the standard format of yyyy.mm.dd. For the values that could not be formatted accordingly, missing values (NaN) were assigned, and these values were substituted with the default date (with the mean year). Extreme values were also removed from this feature column.

The column "Time you went to bed Yesterday" was found to be in different formats, necessitating the need to standardize the format. However, a challenge arose after bringing all values to a standard format, as many values appeared to be in the right format, but semantically did not make sense. For instance, values such as 11.00, 12.00, and 11.30 obviously meant 23.00, 00.00, and 23.30, respectively, given that the question pertained to the time of going to bed. Nevertheless, the program checking for format correctness accepted these values. Therefore, it was imperative to undertake a manual coding process to transform such values to appropriate nighttime hours. Any instances that could not be transformed were marked as missing, and mode imputation was employed to replace them. For most columns, extreme values were removed. For instance, the stress level feature had values ranging from 1 to 100, thus constituting upper and lower boundaries. Similarly, for the question regarding the number of people in the room, the minimum value had to be zero.
In the column regarding the amount of time dedicated to sports, non-numerical values were replaced using mode imputation.

The most challenging features to clean were the responses to open-ended questions: "What program are you in?" and "What makes a good day for you?". These responses
were entirely arbitrary, necessitating categorization. Initially, we utilized natural language processing (NLP) techniques, particularly the NLTK library, to remove any punctuation, tokenize the responses, remove insignificant words (i.e., very small ones), lemmatize them, and stem the remaining words. Additionally, in the "What program are you in?" column instances of "artificial intelligence" were replaced with "ai".

After performing text preprocessing, we were able to identify the most common topics. We used the counter() function to determine the most frequent responses and manually created two label map dictionaries (one for the study program and one for the attributes of a good day). Each dictionary contained keys that corresponded to the most commonly occurring words in the responses, and each key had a set of associated topics.

Using these dictionaries, the algorithm attempted to assign labels to each response, marking any unidentifiable responses as "others" for the study program and "unknown" for the good day attributes. Overall, 49 instances of "other" were recorded for the study program, and 91 instances of "unknown" were recorded for the good day attributes. All other instances of these two features were appropriately labeled.
